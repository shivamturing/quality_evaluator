Evaluate the following prompt for an RL Tool Use Data Generation task.

## Prompt Text
{prompt_text}

## System Context
{system_prompt}

## Expected Tools
{expected_tools}

## Available Tools (Context)
{tool_definitions}

## Auto-Detected Flags
{detected_flags}

---

Please evaluate this prompt against the three criteria:
1. **Naturalness**: Does it sound like a real user request or a developer test case?
2. **Environment Grounding**: Are the entities referenced realistic for this environment?
3. **Complexity**: Does it require actual tool use and state changes?

Scoring rules:
- Use a numeric score from **0 to 5** in **0.5** increments.
- **5** means the criterion fully meets all pass conditions.
- **0** means the criterion completely fails.
- Set `overall_score` to the **minimum** of the three criterion scores.
- Set `overall_pass` to **true only if** `overall_score` is **5**.

Provide your structured JSON evaluation:
```json
{
  "naturalness": {"score": 0-5, "reasoning": "..."},
  "environment_grounding": {"score": 0-5, "reasoning": "..."},
  "complexity": {"score": 0-5, "reasoning": "..."},
  "flags": ["list of any flags detected"],
  "overall_score": 0-5,
  "overall_pass": true,
  "summary": "Brief summary of prompt quality"
}
```
